{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study: Analyzing Flight Delays\n",
    "\n",
    "Now that you've learned how to utilize Dask to read and process large data sets in parallel, you'll put these skills together to search for correlations between flight delays and reported weather events at selected airports. You'll read files in multiple directories containing flight statistics for selected airports from the Bureau of Transportation Statistics and merge them with daily weather data from wunderground.com into a single Dask DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delaying reading & cleaning\n",
    "To work with this subset of the monthly flight information data efficiently, you'll need to do a bit of cleaning. Specifically, you'll need to replace zeros in the 'WEATHER_DELAY' column with nan. This substitution will make counting delays much easier later. This operation requires you to build a delayed pipeline of pandas DataFrame manipulations. You will then convert the output to a Dask DataFrame in which each file will be one chunk.\n",
    "\n",
    "Your first job is to write a function to read a single CSV file into a DataFrame. The DataFrame returned will use pandas TimeStamps in the 'FL_DATE' column, and will have 0s replaced with np.nans in the 'WEATHER_DELAY' column. You can use the flightdelays-2016-1.csv file to verify that the function works as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define @delayed-function read_flights\n",
    "from dask import delayed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "@delayed\n",
    "def read_flights(filename):\n",
    "\n",
    "    # Read in the DataFrame: df\n",
    "    df = pd.read_csv(filename, parse_dates=['FL_DATE'])\n",
    "\n",
    "    # Replace 0s in df['WEATHER_DELAY'] with np.nan\n",
    "    df['WEATHER_DELAY'] = df['WEATHER_DELAY'].replace(0, np.nan)\n",
    "\n",
    "    # Return df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading all flight data\n",
    "A list called filenames is provided for you at the start of this exercise; it contains the strings \"flightdelays-2016-1.csv\" through \"flightdelays-2016-5.csv\". In addition, the delayed function read_flights() defined in the last exercise is provided for you. Also, Numpy & Pandas have been imported for you.\n",
    "\n",
    "Your task now is to iterate over the list filenames and to use the function read_flights to build a list of delayed objects. Finally, you'll concatenate them into a Dask DataFrame with dd.from_delayed() and print out the mean of the WEATHER_DELAY column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.29467680608365\n"
     ]
    }
   ],
   "source": [
    "# modules\n",
    "import dask.dataframe as dd\n",
    "\n",
    "\n",
    "# define filenames\n",
    "filenames = ['flightdelays-2016-1.csv',\n",
    " 'flightdelays-2016-2.csv',\n",
    " 'flightdelays-2016-3.csv',\n",
    " 'flightdelays-2016-4.csv',\n",
    " 'flightdelays-2016-5.csv']\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "# Loop over filenames with index filename\n",
    "for filename in filenames:\n",
    "    # Apply read_flights to filename; append to dataframes\n",
    "    dataframes.append(read_flights(filename))\n",
    "\n",
    "# Compute flight delays: flight_delays\n",
    "flight_delays = dd.from_delayed(dataframes)\n",
    "\n",
    "# Print average of 'WEATHER_DELAY' column of flight_delays\n",
    "print(flight_delays['WEATHER_DELAY'].mean().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deferring reading weather data\n",
    "For this exercise, daily weather data is provided from 2016 for 5 US cities: Atlanta, Denver, Dallas-Fort Worth, Orlando, and Chicago. The weather data comes from Weather Underground and is found in separate CSV files labelled by airport code (e.g., ATL.csv). The list filenames contains the names of these 5 files. The ultimate goal is to correlate the flight delays with weather events from each day of 2016.\n",
    "\n",
    "As with the flight-delays data, you'll need to clean the weather data as it is read in. Your job is to define a function that loads a DataFrame from a file, cleans the DataFrame's 'PrecipitationIn' column, and appends an 'Airport' column with the appropriate airport code for each record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define @delayed\n",
    "@delayed\n",
    "def read_weather (filename):\n",
    "    # Read in filename: df\n",
    "    df = pd.read_csv(filename, parse_dates=['Date'])\n",
    "\n",
    "    # Clean 'PrecipitationIn'\n",
    "    df['PrecipitationIn'] = pd.to_numeric(df['PrecipitationIn'], errors = 'coerce')\n",
    "\n",
    "    # Create the 'Airport' column\n",
    "    df['Airport'] = filename.split('.')[0]\n",
    "\n",
    "    # Return df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a weather DataFrame\n",
    "Your job now is to construct a Dask DataFrame using the function from the previous exercise. To do this, you will iterate over the list filenames provided and build up a list of delayed DataFrames. You'll then concatenate those delayed DataFrames into a Dask DataFrame with dd.from_delayed() as you did with the flight information. Finally, you'll print the row with largest 'Max TemperatureF' value.\n",
    "\n",
    "The list filenames contains the names of the CSV files of weather data labelled by airport code for Atlanta, Denver, Dallas-Fort Worth, Orlando, and Chicago. The read_weather function from the previous exercise is also provided for you and dask.dataframe is imported as dd. Additionally, an empty list called weather_dfs has been created for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Date  Max TemperatureF  Mean TemperatureF  Min TemperatureF  \\\n",
      "224 2016-08-12               107                 93                79   \n",
      "\n",
      "     Max Dew PointF  MeanDew PointF  Min DewpointF  Max Humidity  \\\n",
      "224              75              71             66            79   \n",
      "\n",
      "     Mean Humidity  Min Humidity  ...  Mean VisibilityMiles  \\\n",
      "224             53            27  ...                     8   \n",
      "\n",
      "     Min VisibilityMiles  Max Wind SpeedMPH  Mean Wind SpeedMPH  \\\n",
      "224                    0                 41                  10   \n",
      "\n",
      "     Max Gust SpeedMPH  PrecipitationIn  CloudCover             Events  \\\n",
      "224               54.0             0.82           5  Rain-Thunderstorm   \n",
      "\n",
      "     WindDirDegrees  Airport  \n",
      "224             214      DFW  \n",
      "\n",
      "[1 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# define filenames\n",
    "filenames = ['ATL.csv', 'DEN.csv', 'DFW.csv', 'MCO.csv', 'ORD.csv']\n",
    "\n",
    "weather_dfs = []\n",
    "\n",
    "# Loop over filenames with filename\n",
    "for filename in filenames:\n",
    "    # Invoke read_weather on filename; append result to weather_dfs\n",
    "    weather_dfs.append(read_weather(filename))\n",
    "\n",
    "# Call dd.from_delayed() with weather_dfs: weather\n",
    "weather = dd.from_delayed(weather_dfs)\n",
    "\n",
    "# Print result of weather.nlargest(1, 'Max TemperatureF')\n",
    "print(weather.nlargest(1,'Max TemperatureF').compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which city gets the most snow?\n",
    "The Dask DataFrame weather from the previous exercise is provided here.\n",
    "\n",
    "Your task now is to aggregate the total snow fall for each airport (at least those airports that experienced snow). You'll use the method .str.contains() to create a boolean Series identifying snowy days. You'll need to chain with the method fillna(False) as well; this is to clean NaN values from the boolean Series so it can be used for selection within the .loc[] accessor. After filtering rows that correspond to snowy days from weather, you'll group the rows of the filtered DataFrame by airport code. This allows you to extract the precipitation column and compute aggregated sums grouped by airport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Airport\n",
      "ATL    1.94\n",
      "DEN    5.59\n",
      "ORD    3.91\n",
      "Name: PrecipitationIn, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Make cleaned Boolean Series from weather['Events']: is_snowy\n",
    "is_snowy = weather['Events'].str.contains('Snow').fillna(False)\n",
    "\n",
    "# Create filtered DataFrame with weather.loc & is_snowy: got_snow\n",
    "got_snow = weather.loc[is_snowy]\n",
    "\n",
    "# Groupby 'Airport' column; select 'PrecipitationIn'; aggregate sum(): result\n",
    "result = got_snow.groupby('Airport')['PrecipitationIn'].sum()\n",
    "\n",
    "# Compute & print the value of result\n",
    "print(result.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
