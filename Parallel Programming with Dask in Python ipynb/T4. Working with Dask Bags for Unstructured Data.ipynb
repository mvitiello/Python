{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Dask Bags for Unstructured Data\n",
    "\n",
    "Datasets that have not already been standardized and provided as CSV files can be challenging to work with. In this chapter you'll use the Dask Bag to read raw text files and perform simple text processing workflows over large datasets in parallel. Conceptually, the Dask Bag is a parallel list that can store any Python datatype with convenient functions that map over all of the elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading & counting\n",
    "The sotu/ directory contains text files for each of the 45 US Presidents as of 2017. These text files contain the State of the Union addresses delivered by each president. The texts of the speeches were obtained from the American Presidency Project.\n",
    "\n",
    "The entire speech for each State of the Union address is on a single line in each text file; that is, individual addresses are separated by '\\n'. For example, the 33rd US president Truman delivered 9 State of the Union speeches, so the file sotu/33Truman.txt has 9 lines). Distinct files, then have distinct numbers of lines according to the number of State of the Union addresses each president delivered during their presidency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237\n"
     ]
    }
   ],
   "source": [
    "# import module\n",
    "import glob\n",
    "import dask.bag as db\n",
    "\n",
    "# Glob filenames matching 'sotu/*.txt' and sort\n",
    "filenames = glob.glob('sotu/*.txt')\n",
    "filenames = sorted(filenames)\n",
    "\n",
    "# Load filenames as Dask bag with db.read_text(): speeches\n",
    "speeches = db.read_text(filenames)\n",
    "\n",
    "# Print number of speeches with .count()\n",
    "print(speeches.count().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking one element\n",
    "Let's start using data from the Dask bag speeches of State of the Union addresses. For large datasets, the .take() method is useful for inspecting data & data types while processing dask bags. The value returned by .take(n) is a tuple of the first n elements of the dask.bag.\n",
    "\n",
    "The speeches bag from the previous exercise is provided for you. Your task is to extract the first element of the bag using .take(), to determine its datatype, and to print the words of the first speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      " Fellow-Citizens of the Senate and House of Representatives:\n"
     ]
    }
   ],
   "source": [
    "# Call .take(1): one_element\n",
    "one_element = speeches.take(1)\n",
    "\n",
    "# Extract first element of one_element: first_speech\n",
    "first_speech = one_element[0]\n",
    "\n",
    "# Print type of first_speech and first 60 characters\n",
    "print(type(first_speech))\n",
    "print(first_speech[:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting by word & count\n",
    "Using the speeches bag from earlier exercises let's examine some statistics about the State of the Union addresses.\n",
    "\n",
    "Your job is to split each speech into a list of words using a single space ' ' as the separator. At this point the Dask Bag can be considered a list-of-lists. You'll then map the len() function over each of the inner lists to compute the number of words in each speech and then compute the mean() of the lengths to get the average word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.bag.core.Item'>\n",
      "8239.084388185654\n"
     ]
    }
   ],
   "source": [
    "# Call .str.split(' ') from speeches and assign it to by_word\n",
    "by_word = speeches.str.split(' ')\n",
    "\n",
    "# Map the len function over by_word and compute its mean\n",
    "n_words = by_word.map(len)\n",
    "avg_words = n_words.mean()\n",
    "\n",
    "# Print the type of avg_words and value of avg_words.compute()\n",
    "print(type(avg_words))\n",
    "print(avg_words.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering on a phrase\n",
    "In this exercise you'll make use of the filter function to take the 226 State of the Union addresses and find the addresses where the phrase health care was mentioned. In order to do this you must first standardize the capitalization of all words in each speech.\n",
    "\n",
    "Your job is to convert all speeches to lower case and write a lambda function that returns true if the substring 'health care' is contained in each speech and filter with it. Finally, you'll count the number of speeches retained by the filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "# Convert speeches to lower case: lower\n",
    "lower = speeches.str.lower()\n",
    "\n",
    "# Filter lower for the presence of 'health care': health\n",
    "health = lower.filter(lambda s:'health care' in s)\n",
    "\n",
    "# Count the number of entries : n_health\n",
    "n_health = health.count()\n",
    "\n",
    "# Compute and print the value of n_health\n",
    "print(n_health.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading & mapping from JSON\n",
    "A collection of JSON files for Congressional bills have been downloaded from GovTrack.us. All bills presented during each congressional session from 1973 - 2017 are provided in separate files.\n",
    "\n",
    "Your job is to read the bills*.json files into a Dask Bag. You'll then use the JSON module to map the text of each file into Python dictionaries. You'll then inspect the first element of the Dask Bag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['bill_resolution_type', 'bill_type', 'bill_type_label', 'committee_reports', 'congress', 'current_chamber', 'current_status', 'current_status_date', 'current_status_description', 'current_status_label', 'display_number', 'docs_house_gov_postdate', 'id', 'introduced_date', 'is_alive', 'is_current', 'link', 'lock_title', 'major_actions', 'noun', 'number', 'related_bills', 'scheduled_consideration_date', 'senate_floor_schedule_postdate', 'sliplawnum', 'sliplawpubpriv', 'source', 'source_link', 'sponsor', 'sponsor_role', 'text_incorporation', 'title', 'title_without_number', 'titles'])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Call db.read_text with congress/bills*.json: bills_text\n",
    "bills_text = db.read_text('congress/bills*.json')\n",
    "\n",
    "# Map the json.loads function over all elements: bills_dicts\n",
    "bills_dicts = bills_text.map(json.loads)\n",
    "\n",
    "# Extract the first element with .take(1) and index to the first position: first_bill\n",
    "first_bill = bills_dicts.take(1)[0]\n",
    "\n",
    "# Print the keys of first_bill\n",
    "print(first_bill.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering vetoed bills\n",
    "Now that you've got a Dask Bag prepared with congressional bills as dictionaries we can use filtering and mapping tools to find all of the bills since the 93rd congress that were vetoed by the sitting President and later overridden by congress.\n",
    "\n",
    "The bills_dicts Dask Bag from the previous exercise is provided for you. Your job is to filter the bills to retain those where the current_status key is 'enacted_veto_override'. You'll then print the titles of the bills using .pluck. To help you do this, the following function has been defined for you:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "['H.R. 12471 (93rd): A bill to amend section 552 of title 5, United States Code, known as the Freedom of Information Act.', 'H.R. 6198 (97th): A bill to amend the manufacturing clause of the copyright law.', 'H.R. 6863 (97th): Supplemental Appropriations Act, 1982']\n"
     ]
    }
   ],
   "source": [
    "# Compare the value of the 'current_status' key to 'enacted_veto_override'\n",
    "def veto_override(d):\n",
    "    return d['current_status'] == 'enacted_veto_override'\n",
    "\n",
    "# Filter the bills: overridden\n",
    "overridden = bills_dicts.filter(veto_override)\n",
    "\n",
    "# Print the number of bills retained\n",
    "print(overridden.count().compute())\n",
    "\n",
    "# Get the value of the 'title' key\n",
    "titles = overridden.pluck('title')\n",
    "\n",
    "# Compute and print the titles\n",
    "print(titles.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the average bill's lifespan\n",
    "Some congressional bills can take years to get through committees, floor reading, voting and presidential signatures.\n",
    "\n",
    "Each bill in the bills_dicts Dask Bag has 'current_status_date' and 'introduced_date' keys. Your job is to write a function that returns the number of days that have passed between these two dates. You'll then apply this function over the bills where the 'current_status' is 'enacted_signed'. Finally, you'll compute the average number of days. Pandas has been imported for you as pd and the bills_dicts Dask Bag has been provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function lifespan that takes a dictionary d as input\n",
    "def lifespan(d):\n",
    "    # Convert to datetime\n",
    "    current = pd.to_datetime(d['current_status_date'])\n",
    "    intro = pd.to_datetime(d['introduced_date'])\n",
    "\n",
    "    # Return the number of days\n",
    "    return (current - intro).days\n",
    "\n",
    "# Filter bills_dicts: days\n",
    "days = bills_dicts.filter(lambda s:s['current_status']=='enacted_signed').map(lifespan)\n",
    "\n",
    "# Print the mean value of the days Bag\n",
    "print(days.mean().compute())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
